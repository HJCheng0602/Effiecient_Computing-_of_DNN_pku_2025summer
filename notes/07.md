# Low Rank Decomposition

## Re-visit DNN Pruning

low-rank appriximation : matrix decomposition or tensor decomposition.

![sdfs](../pictures/image%20copy%2019.png)

我们似乎就是把一层的convolution变成了两层的，这样我们就能好压缩和加速了。

## Singular Value Decomposition(SVD)
我们可以对矩阵进行SVD分解，
>SVD分解：任何一个矩阵都可以被写成三个矩阵连乘的形式:$U\Sigma V$,其中$U$是列正交矩阵，$\Sigma$是对角矩阵，$V$是列正交矩阵。且这种拆分方式是唯一的。


通常来讲，我们构建$\Sigma$矩阵的时候，我们把他的对角元通常是从大到小排的。

就像这样：
![sadf](../pictures/image%20copy%2020.png)

然后很清晰的，显然我们把奇异值比较小的给删掉即可


关于SVD的近似误差，我们使用这样的损失计算函数：
$$
||A - B||_F = \sqrt{\sum_{ij}(A_{ij} - B_{ij})^2}
$$

我们定义一个energy量，是所有奇异值的和，然后我们决定r,使得90%的energy被保留。


## 如何计算SVD
好了我们来到了线性代数领域

首先我们需要证明这样的一条结论：
>$AA^{\perp}$和$A^{\perp}A$有相同的非零特征值

然后，我们聚焦于怎么求一个方阵的特征值，由于一元五次方程没有解析解，因此我们尝试去采取另一些方法：

可以考虑幂法求解：
- 首先我们随机选一个任意非零的特征向量
- 然后使用计算：$x_{k + 1} = \frac{M x_k}{||Mx_k||}$,进行迭代
- 最后我们研究$x_k$相比于$x_{k - 1}$的变化。

最后我们就可以这样：（注意$x$是标准向量）
$$
\lambda = x^T M x
$$

来计算特征值了

然后我们继续计算其他特征值，我们可以构造新的矩阵：
$$
M* = M - \lambda x x^T
$$
>注意M是对称矩阵，因此其特征向量两两正交。

所以我们继续拓展SVD的算法：
![sdaf](../pictures/image%20copy%2021.png)

考虑我们SVD算法的复杂度，显然是$O(nm^2)$或$O(mn^2)$，还算能接受的范围

### SVD在DNN中的应用

来看low rank 在conv中的应用：
![dfsaf](../pictures/image%20copy%2022.png)

但是，SVD分解也不能完美解决，因为他是对中间矩阵进行操作，不是对输出结果进行操作，因此他并不能对结果进行太过精确的掌握。


我们可以采取这样的方案：
![sdfsd](../pictures/image%20copy%2023.png)

上面的负责低秩，下面的负责稀疏，大概就是这样的理论：
$$
\min_{A, B}\sum_{i = 1}^{N}||Y_i - r((A + B)X_i)||_F \\
st.||A||_0 \leq S, rank(B)\leq L
$$

但是，这样很容易会产生计算复杂度的爆炸，因此我们可以考虑这样的方案：

![fdsaf](../pictures/image%20copy%2024.png)

其中我们使用*核范数*来近似秩，然后使用了*group lasso*

#### ADMM
还是单独来讲一下什么是ADMM吧

![dsaf](../pictures/image%20copy%2025.png)

增广拉格朗日函数是这样的。

## Tenser Decomposition

