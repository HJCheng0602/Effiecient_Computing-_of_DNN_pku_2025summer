# Pruning

## Sparse Regression (稀疏线性回归)

我们在机器学习的过程中肯定会用到线性回归，我们一般来说需要一个这样的$\beta$来使$\min_\beta ||y - X\beta||_2^2$

作敏感度分析：

$$
S_i = \frac{f(x_1,...,x_i + \Delta x_i, ..., x_K) - f(x_1, ..., x_K)}{\Delta x_i}
$$

如果不敏感的话那么直接删掉就行。


我们也可以进行最小二乘法：
![uiu](../pictures/image%20copy%2013.png)

$$
X\beta = y + y^{\perp}\\
考虑到y是X 列向量的线性组合\\
那么我们就要求y^\perp在X的列向量的正交补空间里即可
$$

- training 
- finetraining

method|Integrated to Learning|Sparsity
|---|---|---|
L0|❌|✅
|L1|✅|✅|
|L2|✅|❌|


我们继续讨论梯度下降的惩罚函数优化算法，考虑这样的lasso损失函数：
$$
\min_xL(x) = \frac{1}{2}(x - \omega)^2 + \lambda |x|
$$

我们分情况讨论，
- 当$x > 0$时，考虑到：
    $$
    L(x) = \frac{1}{2}(x - \omega)^2 + \lambda x
    $$
    我们直接计算：
    $$
    \frac{\partial L}{\partial x} = x - \omega + \lambda = 0
    $$
    我们要意识到一个问题，$\omega$ 是大于零的
-当 $x < 0$时，我们可以退出差不多的一个形式：
    $$
    x = \omega + \lambda < 0
    $$
    注意到要求:$\omega < - \lambda$
- 当x = 0 时,我们可以推出：
    $$
    \omega \in [-\lambda, \lambda]
    $$
    >次梯度

因此，lasso的收敛函数大致如图所示：
![pj](../pictures/image%20copy%2014.png)

**Coordinate descent的核心方法应该就是对变量一个一个地进行优化，显而易见，其要求每个参数之间相互独立**

