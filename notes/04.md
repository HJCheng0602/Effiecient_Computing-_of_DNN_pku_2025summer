# Winograd

好了又是一个一点都不知道是什么的东西www🤔，而且似乎发现教授忘记了昨天他还没讲完Direct COv这张ppt了，听着好难受☹️☹️

## Introduction


卷积事实上只包含两类运算：
- 加法
- 乘法

我们的目标是减少乘法，加法说实话我们并不在意。

显然我们每个人都学过ics 😡
![ui](../pictures/image%20copy%209.png)

上图显而易见乘法的开销很大。

## Strassen

### Matrix Multiplication:

很显然，正常来说这个的朴素的矩阵乘法的复杂度是$O(n^3)$

但是一个想法是我们可以吧矩阵拆成四块，然后递归地进行计算，类似于一种快速幂的方法。递归实现具体如下图所示：

![ui](../pictures/image%20copy%2010.png)

但是，说实话他并不能让我们的复杂度得到下降，大概是这样：
![usidf](../pictures/image%20copy%2011.png)

### Strassen Algorithm

我们follow 时钟频率处理，来了一个莫名其妙的算法：

>Strassen的巧妙之处在于，他发现可以通过增加一些矩阵加法和减法（这些操作的复杂度是 $O(N^2)$），来减少子矩阵乘法的次数。他设计了 7 个中间乘积（就是幻灯片中的$S 
_1
​$
  到$S 
_7
​$
 ），这些乘积本身是子矩阵的乘法。

![pop](../pictures/image%20copy%2012.png)

感觉是一拍脑袋得出来的结果，哎，天赋这一块☹️

最后的复杂度就是：

$$
T(n) = \Theta(N^{\log_2^7}) = \Theta(N^{2.8074})
$$

真TMD艰难

好了教授开始向我们科普顶会了

>AI : ICML  、NewIPS、ICLR
>
>Arch: ISCA、MICRO、HPCA
>
>Vision: CVPR、ICCV、ECCV

但是这个算法局限度很大，因为无法并行化，然后访存有很大问题，评价为走过头了

## Winograd

我们可以把传统的$O(m^2n^2)$复杂度变成了$O(mn)$了

好了具体可以去看这篇论文

> **Andrew Lavin and Scott Gray** (2016). “Fast Algorithms for Convolutional Neural Networks”.
In: Proc. CVPR, pp. 4013–4021.
Winograd Algorithm3


我们发现这个Winograd在实际应用领域并没得到很大的应用，