# 知识蒸馏

## Introduction 

> Knowledge distillation is a model compression method in which a small model is trained to mimic a pre-trained, larger model.


- Knowledge : training dataset $\to$ 
    - Groundtruth
    - Positive/Negative
    - Masked Data
  

![sdfs](../pictures/image%20copy%2031.png)
$$
\mathcal{L}_{KD} = - \sum_{i = 1}^Np(x_i)\log(q(x_i))
$$

## Distillation Method
一共有3中；
- Offline
- Online
- self
  
### KD Scenarios
