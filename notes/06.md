## Kernel Sparse Convolution
我们一般考虑filter是稀疏的

Just like this:

![sdf](../pictures/image%20copy%2015.png)

一个朴素的想法：
```
for all w[i] do
    if w[i] = 0 then
        Continue;
    end if
    output feature map Y <- X * w[i];
end for
```
我们发现这样会导致我们在分支预测上花费很高的开销，因此我们要避免if语句

因此，我们考虑提前把filter存下来，类似于这样：

![fsd](../pictures/image%20copy%2016.png)

我们还是比较喜欢使用csc来进行计算

但是我们考虑csc的话，我们对column进行访问的话，我们对input是进行列访问的，因此起spatial locality并不好。

那么我们其实还可以考虑：$kernel \times Input$,好了我们就比较完美的解决了这个问题。

我们为了更好的稀疏化我们的filter，我们选择使用了**Group Lasso**，我们将$X$分为$X_1, X_2....$，然后我们就可以把我们的损失函数写成了：
$$
\argmin_\beta||y - X\beta||_2^2 + \sum_j\lambda_j||\beta_j||
$$

疑似是一种$L_{1.5}$正则化😂


### Channel Prunning

嗯嗯，就是乘上一个$\beta$来决定哪个通道是否被舍弃，这个矩阵看着好复杂。

### Sparsity-Sparsity Convolution
这是老师一直在回避的内容，他的一位同学在18年的时候研究过这个，确实，理论上来说这个确实能提高效率，但是考虑到DNN里，每一层的input都是上一层的output，因此很难能找到一个原生的支持sparse * sparse之后还会生成sparse的计算方法，因此只能每层对矩阵进行操作，这个开销有点太大了。

### Submanifold Sparse Convolution

似乎ppt上使用了一个hash map来存储非零数值

![sdf](../pictures/image%20copy%2017.png)

这个映射纯煞笔

![fdasf](../pictures/image%20copy%2018.png)


TMD太傻呗了，这就是无语的非常复杂的映射，真tmd煞笔，我宣布发明这个表的就是傻逼😡

这个数据结构狗都不碰，tmd

