# Mo04 Binary / Ternary Network


## Quantization:
- 训练后量化(PTQ)
  
    STE:    我们有向前和向后的传播，我们模拟真实误差，然后backwards
- 量化感知训练(QAT)

    traverse S, check 

## BNN

![fsd](../pictures/image%20copy%2026.png)

我们为了对称性只选择`(-1, 1)`

一张图可以看出其中的区别:

![fdf](../pictures/image%20copy%2027.png)

- 浮点数运算被二进制操作代替(比如说`xor`)
- 二进制权重减小了模型的规模
  
## Main context

### Minimize the Quantization Error
如果我一开始把所有的都存成实数的话，我们需要加减乘除，那么我们考虑使用二进制存储？

|||||
|---|---|---|---|
|$\mathbb{R} \times \mathbb{R}$|+ - $\times$| 1x|1x|
|$\mathbb{R} \times \mathbb{B}$|+ -|~32x|~2x|d
|$\mathbb{B} \times \mathbb{B}$|XNOR  Bitcount|~32x|~58x|

二值化的方法大概是这样：

![fdsa](../pictures/image%20copy%2028.png)

我们接下来讨论一下这个是怎么样算出来的。

**注意，这不是一种loss函数，而是一种损失较小的量化方式**

首先：
$$
J(B,\alpha) = ||W - \alpha B||^2 = \alpha ^2 B^TB - 2\alpha W^TB + W^TW
$$

首先我们可以意识到$B^TB$是一个定值，当且仅当在BNN里。

我们将这个对于$\alpha$求偏导，很显然：
$$
\alpha^* = \frac{W^TB^*}{n}
$$

然后我们再把$B^* = sign(W)$带入即可。

因此其伪代码如下：
![fds](../pictures/image%20copy%2029.png)

上面是表格里的$\mathbb{R} \times \mathbb{{B}}$的操作
,下面我们讨论一下$\mathbb{B} \times \mathbb{B}$的方法，反正大体是相似的。
![fasdf](../pictures/image%20copy%2030.png)